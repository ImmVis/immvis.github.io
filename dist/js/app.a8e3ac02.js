(function(){"use strict";var e={6364:function(e,n,a){var t=a(9242),i=a(5068),o=(a(2166),a(3396));function r(e,n,a,t,i,r){const s=(0,o.up)("HeaderBar"),l=(0,o.up)("router-view"),c=(0,o.up)("FooterBar");return(0,o.wg)(),(0,o.iD)(o.HY,null,[(0,o.Wm)(s),(0,o.Wm)(l),(0,o.Wm)(c)],64)}var s=a(7139);const l=e=>((0,o.dD)("data-v-5b26d1f4"),e=e(),(0,o.Cn)(),e),c={class:"navbar navbar-expand-sm navbar-light bg-light"},d={class:"container-fluid"},u=l((()=>(0,o._)("button",{class:"navbar-toggler",type:"button","data-bs-toggle":"collapse","data-bs-target":"#navbarSupportedContent","aria-controls":"navbarSupportedContent","aria-expanded":"false","aria-label":"Toggle navigation"},[(0,o._)("span",{class:"navbar-toggler-icon"})],-1))),m={class:"collapse navbar-collapse",id:"navbarSupportedContent"},p={class:"navbar-nav me-auto mb-2 mb-sm-0"};function g(e,n,a,t,i,r){const l=(0,o.up)("router-link");return(0,o.wg)(),(0,o.iD)("nav",c,[(0,o._)("div",d,[u,(0,o._)("div",m,[(0,o._)("ul",p,[((0,o.wg)(!0),(0,o.iD)(o.HY,null,(0,o.Ko)(i.pages,(n=>((0,o.wg)(),(0,o.iD)("li",{key:n.route,class:"nav-item"},[(0,o.Wm)(l,{to:{name:n.route},class:(0,s.C_)(["nav-link",{active:e.$route.name==n.route,disabled:!n.enabled}])},{default:(0,o.w5)((()=>[(0,o.Uk)((0,s.zw)(n.title),1)])),_:2},1032,["to","class"])])))),128))])])])])}var h={data(){return{pages:[{route:"Home",title:"ImmVis",enabled:!0},{route:"Staff",title:"Staff",enabled:!1},{route:"Research",title:"Research",enabled:!1},{route:"Courses",title:"Courses",enabled:!1},{route:"ExjobbList",title:"ExjobbList",enabled:!0}]}}},f=a(89);const v=(0,f.Z)(h,[["render",g],["__scopeId","data-v-5b26d1f4"]]);var b=v;const w=e=>((0,o.dD)("data-v-650ffc63"),e=e(),(0,o.Cn)(),e),y={class:"navbar navbar-expand-sm navbar-light"},_={class:"container-fluid"},k=w((()=>(0,o._)("button",{class:"navbar-toggler",type:"button","data-bs-toggle":"collapse","data-bs-target":"#navbarSupportedContent","aria-controls":"navbarSupportedContent","aria-expanded":"false","aria-label":"Toggle navigation"},[(0,o._)("span",{class:"navbar-toggler-icon"})],-1)));function j(e,n,a,t,i,r){const l=(0,o.up)("router-link");return(0,o.wg)(),(0,o.iD)("nav",y,[(0,o._)("div",_,[(0,o.Wm)(l,{to:{name:"Home"},class:(0,s.C_)(["navbar-brand",{active:e.$route.name==e.name}])},{default:(0,o.w5)((()=>[(0,o.Uk)(" ImmVis ")])),_:1},8,["class"]),k])])}var D={data(){return{}}};const x=(0,f.Z)(D,[["render",j],["__scopeId","data-v-650ffc63"]]);var C=x,T={name:"App",components:{HeaderBar:b,FooterBar:C}};const B=(0,f.Z)(T,[["render",r]]);var z=B,I=a(2483);const S=e=>((0,o.dD)("data-v-8bff1012"),e=e(),(0,o.Cn)(),e),E={class:"container"},M=S((()=>(0,o._)("h1",null,"ImmVis",-1))),R=S((()=>(0,o._)("p",null,"Under construction",-1))),H=S((()=>(0,o._)("hr",null,null,-1)));function P(e,n,a,t,i,r){const s=(0,o.up)("BannerBar"),l=(0,o.up)("router-link");return(0,o.wg)(),(0,o.iD)(o.HY,null,[(0,o.Wm)(s,{title:"Immersive Visualization"}),(0,o._)("div",E,[M,R,H,(0,o.Wm)(l,{to:{name:"ExjobbList"},class:""},{default:(0,o.w5)((()=>[(0,o.Uk)(" Click here to view all available exjobbs. ")])),_:1})])],64)}const F={class:"jumbotron p-4 mb-5"},Y={class:"container py-5"},L={class:"display-5 text-white"};function A(e,n,a,t,i,r){return(0,o.wg)(),(0,o.iD)("div",F,[(0,o._)("div",Y,[(0,o._)("h1",L,(0,s.zw)(a.title),1)])])}var W={name:"BannerBar",props:["title"]};const N=(0,f.Z)(W,[["render",A],["__scopeId","data-v-21e1b322"]]);var O=N,V={name:"HomePage",components:{BannerBar:O},props:{msg:String}};const q=(0,f.Z)(V,[["render",P],["__scopeId","data-v-8bff1012"]]);var J=q;function Z(e,n,a,t,i,o){return" STAFF PAGE "}var K={name:"StaffPage",props:{msg:String}};const U=(0,f.Z)(K,[["render",Z]]);var G=U;function $(e,n,a,t,i,o){return" RESEARCH PAGE "}var Q={name:"ResearchPage",props:{msg:String}};const X=(0,f.Z)(Q,[["render",$]]);var ee=X;function ne(e,n,a,t,i,o){return" COURSES PAGE "}var ae={name:"CoursesPage",props:{msg:String}};const te=(0,f.Z)(ae,[["render",ne]]);var ie=te;const oe=e=>((0,o.dD)("data-v-acd93c08"),e=e(),(0,o.Cn)(),e),re={class:"container"},se=oe((()=>(0,o._)("h1",null,"Exjobb",-1))),le={class:"table"},ce=oe((()=>(0,o._)("thead",null,[(0,o._)("tr",null,[(0,o._)("th",{scope:"col"},"Title"),(0,o._)("th",{scope:"col"},"Location"),(0,o._)("th",{scope:"col"},"Students")])],-1)));function de(e,n,a,t,i,r){const l=(0,o.up)("BannerBar"),c=(0,o.up)("router-link");return(0,o.wg)(),(0,o.iD)(o.HY,null,[(0,o.Wm)(l,{title:"Immersive Visualization"}),(0,o._)("div",re,[se,(0,o._)("table",le,[ce,(0,o._)("tbody",null,[((0,o.wg)(!0),(0,o.iD)(o.HY,null,(0,o.Ko)(i.projectList,((e,n)=>((0,o.wg)(),(0,o.iD)("tr",{key:n},[(0,o._)("td",null,[(0,o.Wm)(c,{to:{name:"Exjobb",params:{jobbId:e.id}}},{default:(0,o.w5)((()=>[(0,o.Uk)((0,s.zw)(e.title),1)])),_:2},1032,["to"])]),(0,o._)("td",null,(0,s.zw)(e.location),1),(0,o._)("td",null,(0,s.zw)(e.students),1)])))),128))])])])],64)}const ue="\n#### Description\n\nVideo streams constitute a large part of the daily internet traffic. A one hour\nlong video at 4K resolution and 25 frames per second requires about 2TB of\nstorage if no compression is applied. As a result, it is of utmost need to find so-\nlutions to intelligently transfer/use such large amounts of data. Modern video\ncodecs have enabled the streaming of video data over the internet, in real-time,\ne.g., in a video call, or as demanded, such as YouTube video streaming or Netflix\nmovies. In recent years, there have been some attempts to the standardization\nof machine learning approaches in video codecs such as MPEG video coding\nfor machine (VCM) standards for machine-to-machine (M2M) or machine-to-\nhuman (M2H) communications, as well as JPEG AI, and JVET Neural Network\nVideo Coding (NNVC). This project aims to employ an unsupervised machine\nlearning approach for encoding and decoding a video using sparse represen-\ntations and applying fast and accurate quantization and entropy coding on the\nresulting sparse coefficients.\n\n\n#### Your tasks\n\nExplore using machine learning methods to develop a codec for video stream-\ning. The codec consists of both an encoder and a decoder. You will use an unsu-\npervised machine learning method, named AMDE, to learn a sparse represen-\ntation of the dataset from a training set. The video frames are then transformed\ninto sparse coefficients which are then quantized and further compressed us-\ning an entropy coding algorithm such as Huffman coding. You will carry out\nan analysis of the quality of the codec in terms of compression efficiency and\nencoding latency in comparison with state-of-the-art video codec approaches.\nThe source code and required tools for utilizing AMDE will be provided.\n\n\n#### Your profile\n\nWe are looking for 1 student with an interest in machine learning, image pro-\ncessing, and computer graphics.\n\n\n#### Information\n\nResearch group: Computer graphics and image processing\n\nContact person: Saghi Hajisharif and Ehsan Miandji\n\nLocation: The division for Media and Information Technology, Campus Norrköping\n\nKeywords:\n* machine learning\n* image processing\n* compression\n* rendering\n\nLevel: Master\n";var me={id:"hajisharif_miandji_video_compression",title:"A learning-based video compression with sparse representation and entropy coding",description:"",contact:[{name:"Saghi Hajisharif",url:"https://liu.se/medarbetare/sagha08",image:"https://liu.se/-/media/employeeimages/08/employee_image_sagha08.jpeg"},{name:"Ehsan Miandji",url:"https://liu.se/medarbetare/miaeh27",image:"https://liu.se/-/media/employeeimages/27/employee_image_miaeh27.jpeg"}],location:"Norrköping",students:1,image:null,markdown:ue},pe=a.p+"img/brdf_render.5d14ffbf.png";const ge="\n#### Description\n\nPhoto-realistic rendering requires accurate modeling of the appearance of real-\nworld materials using the bidirectional reflectance distribution function (BRDF).\nThere are various ways to model BRDFs, and in practice due to their compact\nand flexible form, analytic BRDF models are often employed to estimate the sur-\nface properties. However, these models despite being efficient for rendering,\nare not very realistic. Measured BRDFs on the other hand can accurately model\na realistic appearance, but they are often computationally expensive and con-\nsume significantly more memory, which makes them impractical for real-world\napplications. It has been shown, however, with sparse modeling of measured\nBRDFs, a non-parametric model can be defined that reduces the dimension-\nality of the BRDF, and therefore the rendering cost. Sparse modeling enables\nrendering speeds competitive with analytical models while admitting realistic\nmodeling of BRDFs.\n\n\n#### Your tasks\n\nYou will explore how non-parametric sparse BRDF modeling can be utilized for\nrealistic rendering. You will modify an existing ray tracer such as PBRT/Mitsuba\nor write your own ray tracer to employ the non-parametric BRDF model and\nanalyze the capability of this model for fast and realistic rendering. The source\ncode and required tools for sparse BRDF modeling is available. An analysis\nof how the parameters of sparse modeling affect the quality and efficiency of\nrendering is required as well.\n\n\n#### Your profile\n\nWe are looking for 1-2 students with a background in machine learning and\ncomputer graphics.\n\n\n#### Information\n\nResearch group: Computer graphics and image processing\n\nContact person: Saghi Hajisharif and Ehsan Miandji\n\nLocation: The division for Media and Information Technology, Campus Norrköping\n\nKeywords:\n* machine learning\n* rendering\n* BRDF\n* sparse representations\n\nLevel: Master\n";var he={id:"hajisharif_miandji_learning_based_rendering",title:"Learning-based rendering using a data-driven BRDF model",description:"",contact:[{name:"Saghi Hajisharif",url:"https://liu.se/medarbetare/sagha08",image:"https://liu.se/-/media/employeeimages/08/employee_image_sagha08.jpeg"},{name:"Ehsan Miandji",url:"https://liu.se/medarbetare/miaeh27",image:"https://liu.se/-/media/employeeimages/27/employee_image_miaeh27.jpeg"}],location:"Norrköping",students:1,image:pe,markdown:ge},fe=a.p+"img/neural.22e24627.png";const ve="\n#### Description\n\nFeature visualization answers questions about what a neural network — or\nparts of a network — are looking for by generating examples. If we want to\nunderstand individual features, we can search for examples where they have\nhigh values as exemplified in the images above. However, they can be hard to\ninterpret and the correctness of current implementations are questionable due\nto a lack of ground truth comparisons. Furthermore, the initial implementation\nfor the technique is based on the outdated Tensorflow 1, which does not even\nrun anymore.\n\n\n#### Your tasks\n\nInvestigate different techniques for generating feature visualizations. For ex-\nample, by optimizing for a given set of values instead of only the high values.\nAlso try to create verifiable visualizations that can serve as a basis for Tensor-\nflow 2 and/or PyTorch implementations.\n\n\n#### Your profile\n\nWe are looking for 1 student with an interest in machine learning and visualiza-\ntion.\n\n\n#### Information\n\nResearch group: Computer graphics and image processing\n\nContact person: Daniel Jönsson\n\nLocation: The division for Media and Information Technology, Campus Norrköping\n\nKeywords:\n* machine learning\n* visualization\n\nLevel: Master\n";var be={id:"jonsson_neural_network_visualization",title:"Neural network feature visualization",description:"",contact:[{name:"Daniel Jönsson",url:"https://liu.se/medarbetare/danjo37",image:"https://liu.se/-/media/employeeimages/37/employee_image_danjo37.jpeg"}],location:"Norrköping",students:1,image:fe,markdown:ve};const we="\n#### Description\n\nDesigning, creating, and studying are all iterative, reflective process and we are interested in recording, organising, and visually exploring the artifacts and notes that are generated during these processes. We are looking for a MS student interested in diving into this design and engineering space to implement an interactive web-based visualization tool to support such reflection, navigation, and reporting of iterative practices. You will work with the APIs of cloud-based note-taking services like Notion or Evernote and js languages like d3 and Vue/React. We will work iteratively to develop the tool with design and visualization best practices. This project will take place in collaboration with students and faculty in the new Vis Collective research lab.\n";var ye={id:"akbaba_tracing_research_insights",title:"Tracing Research Insights",description:"Designing, creating, and studying are all iterative, reflective process and we are interested in recording, organising, and visually exploring the artifacts and notes that are generated during these processes. We are looking for a MS student interested in diving into this design and engineering space to implement an interactive web-based visualization tool to support such reflection, navigation, and reporting of iterative practices. You will work with the APIs of cloud-based note-taking services like Notion or Evernote and js languages like d3 and Vue/React. We will work iteratively to develop the tool with design and visualization best practices. This project will take place in collaboration with students and faculty in the new Vis Collective research lab.",contact:[{name:"Derya Akbaba",url:"https://liu.se/en/employee/derak07",image:"https://liu.se/-/media/employeeimages/07/employee_image_derak07.jpeg"}],location:"Norrköping",students:1,image:null,markdown:we},_e=[me,he,be,ye],ke={name:"ExjobbListPage",components:{BannerBar:O},data(){return{projectList:_e}}};const je=(0,f.Z)(ke,[["render",de],["__scopeId","data-v-acd93c08"]]);var De=je;const xe=e=>((0,o.dD)("data-v-19fcb59c"),e=e(),(0,o.Cn)(),e),Ce={class:"container"},Te={class:"row"},Be={class:"col-6 p-2"},ze={class:"p-3"},Ie=xe((()=>(0,o._)("h5",null,"Location",-1))),Se=xe((()=>(0,o._)("h5",null,"Number of people",-1))),Ee=xe((()=>(0,o._)("h5",null,"Contact",-1))),Me=["src"],Re=["href"],He={class:"col-6 p-2 d-flex align-items-center image-box"},Pe={class:"p-3"},Fe=["src"],Ye={key:1,src:"https://picsum.photos/800/200"},Le=xe((()=>(0,o._)("hr",{class:"opacity-0 my-4"},null,-1))),Ae=["innerHTML"];function We(e,n,a,t,i,r){const l=(0,o.up)("BannerBar");return(0,o.wg)(),(0,o.iD)(o.HY,null,[(0,o.Wm)(l,{title:"Immersive Visualization"}),(0,o._)("div",Ce,[(0,o._)("h1",null,(0,s.zw)(r.myJobb.title),1),(0,o._)("div",Te,[(0,o._)("div",Be,[(0,o._)("div",ze,[Ie,(0,o._)("p",null,(0,s.zw)(r.myJobb.location),1),Se,(0,o._)("p",null,(0,s.zw)(r.myJobb.students),1),Ee,(0,o._)("ul",null,[((0,o.wg)(!0),(0,o.iD)(o.HY,null,(0,o.Ko)(r.myJobb.contact,(e=>((0,o.wg)(),(0,o.iD)("li",{key:e.name,class:"contact"},[(0,o._)("img",{src:e.image},null,8,Me),(0,o._)("a",{href:e.url},(0,s.zw)(e.name),9,Re)])))),128))])])]),(0,o._)("div",He,[(0,o._)("div",Pe,[r.myJobb.image?((0,o.wg)(),(0,o.iD)("img",{key:0,src:r.myJobb.image},null,8,Fe)):((0,o.wg)(),(0,o.iD)("img",Ye))])])]),Le,(0,o._)("div",{innerHTML:r.markdown},null,8,Ae)])],64)}var Ne={name:"ExjobbPage",props:["jobbId"],components:{BannerBar:O},data(){return{}},computed:{myJobb(){return _e.find((e=>e.id==this.jobbId))},markdown(){return this.md(this.myJobb.markdown)}}};const Oe=(0,f.Z)(Ne,[["render",We],["__scopeId","data-v-19fcb59c"]]);var Ve=Oe;const qe=[{path:"/",name:"Home",component:J},{path:"/staff",name:"Staff",component:G},{path:"/research",name:"Research",component:ee},{path:"/courses",name:"Courses",component:ie},{path:"/exjobb-mit",name:"ExjobbList",component:De},{path:"/exjobb-mit/:jobbId([a-z0-9_]+)",name:"Exjobb",component:Ve,props:!0}],Je=(0,I.p7)({mode:"history",history:(0,I.PO)(),routes:qe});var Ze=Je;const Ke={methods:{md:e=>(0,i.TU)(e)}},Ue=(0,t.ri)(z);Ue.use(Ze),Ue.mixin(Ke),Ue.mount("#app")}},n={};function a(t){var i=n[t];if(void 0!==i)return i.exports;var o=n[t]={exports:{}};return e[t](o,o.exports,a),o.exports}a.m=e,function(){var e=[];a.O=function(n,t,i,o){if(!t){var r=1/0;for(d=0;d<e.length;d++){t=e[d][0],i=e[d][1],o=e[d][2];for(var s=!0,l=0;l<t.length;l++)(!1&o||r>=o)&&Object.keys(a.O).every((function(e){return a.O[e](t[l])}))?t.splice(l--,1):(s=!1,o<r&&(r=o));if(s){e.splice(d--,1);var c=i();void 0!==c&&(n=c)}}return n}o=o||0;for(var d=e.length;d>0&&e[d-1][2]>o;d--)e[d]=e[d-1];e[d]=[t,i,o]}}(),function(){a.n=function(e){var n=e&&e.__esModule?function(){return e["default"]}:function(){return e};return a.d(n,{a:n}),n}}(),function(){a.d=function(e,n){for(var t in n)a.o(n,t)&&!a.o(e,t)&&Object.defineProperty(e,t,{enumerable:!0,get:n[t]})}}(),function(){a.g=function(){if("object"===typeof globalThis)return globalThis;try{return this||new Function("return this")()}catch(e){if("object"===typeof window)return window}}()}(),function(){a.o=function(e,n){return Object.prototype.hasOwnProperty.call(e,n)}}(),function(){a.r=function(e){"undefined"!==typeof Symbol&&Symbol.toStringTag&&Object.defineProperty(e,Symbol.toStringTag,{value:"Module"}),Object.defineProperty(e,"__esModule",{value:!0})}}(),function(){a.p="/"}(),function(){var e={143:0};a.O.j=function(n){return 0===e[n]};var n=function(n,t){var i,o,r=t[0],s=t[1],l=t[2],c=0;if(r.some((function(n){return 0!==e[n]}))){for(i in s)a.o(s,i)&&(a.m[i]=s[i]);if(l)var d=l(a)}for(n&&n(t);c<r.length;c++)o=r[c],a.o(e,o)&&e[o]&&e[o][0](),e[o]=0;return a.O(d)},t=self["webpackChunkimmvis"]=self["webpackChunkimmvis"]||[];t.forEach(n.bind(null,0)),t.push=n.bind(null,t.push.bind(t))}();var t=a.O(void 0,[998],(function(){return a(6364)}));t=a.O(t)})();
//# sourceMappingURL=app.a8e3ac02.js.map